{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vFL1kgo-xAS"
      },
      "source": [
        "Установки всего нужного"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kBMt8jcQg7K",
        "outputId": "fac1a8a4-0e29-4563-cb5c-6c990cc120d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50aBzmYjQjGh",
        "outputId": "712cbcc6-bba8-4f5a-e122-5fc2d0e1836b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qaglhk0KQnDc",
        "outputId": "5187f77f-8844-4eeb-c859-578e14943585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh87dt1HQ4fL",
        "outputId": "b75a7986-8e67-471c-d62c-93fafa07e946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nDQuVJmhQahv"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import datasets\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import ClassLabel, load_dataset\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    PreTrainedTokenizerFast,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version, send_example_telemetry\n",
        "from transformers.utils.versions import require_version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzhSS6Y5YA3X",
        "outputId": "59a20917-38f9-4c0b-f84e-8351efe88e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.28.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiHWFdCVFmw9",
        "outputId": "4006edb0-8dad-4956-a31d-b81dc4c7ac37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.3.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=be697711d3f1da2b914739eb4141181b663e9376a3dd909949232615127f11b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZgFN8bQH08s"
      },
      "source": [
        "# Эксперименты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJU6ceXUmwTn",
        "outputId": "ebbb0cd8-17ea-47d3-af80-e0c61ec114bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 12:54:03.054572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 12:54:03.054624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 12:54:03.056270: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 12:54:04.961251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/24/2024 12:54:08 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "Generating train split: 1658 examples [00:00, 38754.77 examples/s]\n",
            "Generating validation split: 292 examples [00:00, 51424.96 examples/s]\n",
            "config.json: 100% 1.19k/1.19k [00:00<00:00, 6.45MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Babelscape/wikineural-multilingual-ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 333/333 [00:00<00:00, 1.36MB/s]\n",
            "vocab.txt: 100% 996k/996k [00:01<00:00, 616kB/s]\n",
            "tokenizer.json: 100% 1.96M/1.96M [00:00<00:00, 7.55MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 473kB/s]\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/tokenizer_config.json\n",
            "model.safetensors: 100% 709M/709M [00:46<00:00, 15.1MB/s]\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Babelscape/wikineural-multilingual-ner and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 1658/1658 [00:00<00:00, 3014.29 examples/s]\n",
            "Running tokenizer on dataset: 100% 292/292 [00:00<00:00, 2934.23 examples/s]\n",
            "03/24/2024 12:55:07 - INFO - __main__ - Sample 236 of the training set: {'input_ids': [101, 10167, 10134, 10478, 46503, 18103, 10284, 20048, 45825, 96508, 10140, 13173, 22394, 10681, 190, 10115, 100, 10140, 32597, 10223, 15826, 10143, 10305, 10298, 46503, 41565, 10123, 11615, 10115, 190, 10115, 100, 17094, 15834, 10870, 10269, 61001, 12587, 10143, 10941, 10129, 13173, 10237, 10525, 46963, 10143, 72589, 13733, 10372, 14716, 27424, 10112, 189, 11030, 52321, 10350, 101826, 29420, 71843, 14260, 19285, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, 0, -100, -100, 0, -100, -100, 0, 0, 0, 0, 0, -100, 0, 0, -100, -100, 0, -100, 0, -100, -100, 0, -100, -100, -100, 0, -100, 0, 0, -100, 0, -100, -100, -100, 0, 0, -100, 0, 0, -100, -100, 0, -100, 0, -100, 0, -100, -100, -100, 0, -100]}.\n",
            "03/24/2024 12:55:07 - INFO - __main__ - Sample 434 of the training set: {'input_ids': [101, 11741, 74052, 10486, 10134, 10380, 56883, 10223, 10478, 45825, 16266, 10123, 46503, 39854, 10123, 10211, 28004, 10143, 10305, 37881, 14401, 72631, 190, 10115, 100, 30409, 13770, 37948, 13173, 10976, 10139, 73336, 10129, 15182, 10115, 10104, 85980, 13319, 62644, 51282, 15331, 11244, 10269, 11471, 11382, 10795, 10338, 10123, 21131, 10414, 13599, 13484, 10246, 187, 15577, 13319, 10817, 10166, 10268, 187, 73267, 10870, 31826, 10161, 12127, 11244, 10151, 10745, 24984, 11280, 176, 10477, 10965, 10632, 10294, 10237, 10215, 164, 32221, 11244, 10269, 11471, 10305, 36124, 10815, 46503, 73918, 10115, 11741, 74052, 10486, 10149, 46503, 104216, 10294, 174, 74052, 10681, 39312, 76486, 175, 36220, 10136, 190, 10115, 100, 10485, 11666, 46503, 104216, 10294, 98696, 10136, 80140, 10877, 12337, 10106, 10143, 10305, 106482, 43125, 53543, 10123, 12228, 10123, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 1, -100, -100, 0, 0, 0, 0, 0, 0, 0, -100, 0, -100, -100, 0, 0, 0, -100, 0, -100, -100, 0, -100, -100, 1, -100, -100, 0, -100, 0, 0, -100, 0, -100, 0, -100, 0, 0, -100, 0, -100, -100, 0, -100, 0, 0, -100, 0, 0, -100, 0, -100, 0, -100, 0, 0, 0, 0, 0, -100, -100, 0, -100, 0, -100, 0, 0, 0, -100, 0, -100, -100, 0, 0, -100, 0, 0, 0, -100, -100, 0, -100, 0, -100, 0, -100, -100, 1, -100, -100, 0, 0, -100, 0, 0, -100, -100, 0, 0, 0, -100, -100, 0, -100, -100, 0, -100, 0, -100, 0, 0, -100, 0, -100, -100, 0, 0, -100, 1, -100, -100, -100, 0, -100, -100]}.\n",
            "03/24/2024 12:55:07 - INFO - __main__ - Sample 399 of the training set: {'input_ids': [101, 11791, 109015, 10294, 19911, 13319, 33264, 10112, 12555, 21941, 10269, 10134, 10478, 181, 63129, 10140, 28780, 17892, 10140, 176, 11244, 10410, 10294, 31971, 76486, 16363, 10112, 20722, 10308, 175, 63129, 10294, 187, 14394, 10269, 10106, 50302, 16822, 15186, 12337, 10380, 70324, 10166, 10478, 10215, 10143, 10305, 10795, 10128, 18643, 23041, 10104, 34447, 10493, 10112, 191, 16497, 10115, 11941, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 0, -100, 0, -100, -100, 0, 0, 0, -100, 0, 0, -100, 0, 0, -100, -100, 0, 0, 0, 0, -100, 0, -100, 0, -100, 0, 0, -100, -100, 0, 0, -100, -100, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, -100, 0, -100, 0, -100, 0, -100, -100, 0, -100]}.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:571: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 20.8MB/s]\n",
            "03/24/2024 12:55:09 - INFO - __main__ - ***** Running training *****\n",
            "03/24/2024 12:55:09 - INFO - __main__ -   Num examples = 1658\n",
            "03/24/2024 12:55:09 - INFO - __main__ -   Num Epochs = 3\n",
            "03/24/2024 12:55:09 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
            "03/24/2024 12:55:09 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "03/24/2024 12:55:09 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "03/24/2024 12:55:09 - INFO - __main__ -   Total optimization steps = 156\n",
            " 33% 52/156 [00:32<01:05,  1.59it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch 0: {'precision': 0.7908611599297012, 'recall': 0.907258064516129, 'f1': 0.8450704225352113, 'accuracy': 0.9902535657686212}\n",
            " 67% 104/156 [01:07<00:33,  1.54it/s]epoch 1: {'precision': 0.9153846153846154, 'recall': 0.9596774193548387, 'f1': 0.9370078740157479, 'accuracy': 0.9958003169572108}\n",
            "100% 156/156 [01:44<00:00,  1.45it/s]epoch 2: {'precision': 0.9174664107485605, 'recall': 0.9637096774193549, 'f1': 0.9400196656833825, 'accuracy': 0.995958795562599}\n",
            "Configuration saved in /content/our_output/config.json\n",
            "Model weights saved in /content/our_output/model.safetensors\n",
            "tokenizer config file saved in /content/our_output/tokenizer_config.json\n",
            "Special tokens file saved in /content/our_output/special_tokens_map.json\n",
            "100% 156/156 [01:50<00:00,  1.41it/s]\n"
          ]
        }
      ],
      "source": [
        "!python ner.py \\\n",
        "  --model_name_or_path Babelscape/wikineural-multilingual-ner \\\n",
        "  --train_file /content/ner_dataset_train.pkl \\\n",
        "  --validation_file /content/ner_dataset_eval.pkl \\\n",
        "  --text_column_name tokens \\\n",
        "  --label_column_name ner_tags \\\n",
        "  --task_name ner \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /content/our_output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju_uc6M_w2zs",
        "outputId": "f1c2b129-90c8-4bd6-f540-b6d9a4a2bffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 12:58:26.342864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 12:58:26.342916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 12:58:26.344114: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 12:58:27.518886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/24/2024 12:58:32 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "config.json: 100% 1.53k/1.53k [00:00<00:00, 5.65MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fhswf/bert_de_ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 62.0/62.0 [00:00<00:00, 241kB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fhswf/bert_de_ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-LOC\",\n",
            "    \"2\": \"B-LOCderiv\",\n",
            "    \"3\": \"B-LOCpart\",\n",
            "    \"4\": \"B-ORG\",\n",
            "    \"5\": \"B-ORGderiv\",\n",
            "    \"6\": \"B-ORGpart\",\n",
            "    \"7\": \"B-OTH\",\n",
            "    \"8\": \"B-OTHderiv\",\n",
            "    \"9\": \"B-OTHpart\",\n",
            "    \"10\": \"B-PER\",\n",
            "    \"11\": \"B-PERderiv\",\n",
            "    \"12\": \"B-PERpart\",\n",
            "    \"13\": \"I-LOC\",\n",
            "    \"14\": \"I-LOCderiv\",\n",
            "    \"15\": \"I-LOCpart\",\n",
            "    \"16\": \"I-ORG\",\n",
            "    \"17\": \"I-ORGderiv\",\n",
            "    \"18\": \"I-ORGpart\",\n",
            "    \"19\": \"I-OTH\",\n",
            "    \"20\": \"I-OTHderiv\",\n",
            "    \"21\": \"I-OTHpart\",\n",
            "    \"22\": \"I-PER\",\n",
            "    \"23\": \"I-PERderiv\",\n",
            "    \"24\": \"I-PERpart\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 1,\n",
            "    \"B-LOCderiv\": 2,\n",
            "    \"B-LOCpart\": 3,\n",
            "    \"B-ORG\": 4,\n",
            "    \"B-ORGderiv\": 5,\n",
            "    \"B-ORGpart\": 6,\n",
            "    \"B-OTH\": 7,\n",
            "    \"B-OTHderiv\": 8,\n",
            "    \"B-OTHpart\": 9,\n",
            "    \"B-PER\": 10,\n",
            "    \"B-PERderiv\": 11,\n",
            "    \"B-PERpart\": 12,\n",
            "    \"I-LOC\": 13,\n",
            "    \"I-LOCderiv\": 14,\n",
            "    \"I-LOCpart\": 15,\n",
            "    \"I-ORG\": 16,\n",
            "    \"I-ORGderiv\": 17,\n",
            "    \"I-ORGpart\": 18,\n",
            "    \"I-OTH\": 19,\n",
            "    \"I-OTHderiv\": 20,\n",
            "    \"I-OTHpart\": 21,\n",
            "    \"I-PER\": 22,\n",
            "    \"I-PERderiv\": 23,\n",
            "    \"I-PERpart\": 24,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "vocab.txt: 100% 240k/240k [00:00<00:00, 44.7MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 396kB/s]\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fhswf/bert_de_ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-LOC\",\n",
            "    \"2\": \"B-LOCderiv\",\n",
            "    \"3\": \"B-LOCpart\",\n",
            "    \"4\": \"B-ORG\",\n",
            "    \"5\": \"B-ORGderiv\",\n",
            "    \"6\": \"B-ORGpart\",\n",
            "    \"7\": \"B-OTH\",\n",
            "    \"8\": \"B-OTHderiv\",\n",
            "    \"9\": \"B-OTHpart\",\n",
            "    \"10\": \"B-PER\",\n",
            "    \"11\": \"B-PERderiv\",\n",
            "    \"12\": \"B-PERpart\",\n",
            "    \"13\": \"I-LOC\",\n",
            "    \"14\": \"I-LOCderiv\",\n",
            "    \"15\": \"I-LOCpart\",\n",
            "    \"16\": \"I-ORG\",\n",
            "    \"17\": \"I-ORGderiv\",\n",
            "    \"18\": \"I-ORGpart\",\n",
            "    \"19\": \"I-OTH\",\n",
            "    \"20\": \"I-OTHderiv\",\n",
            "    \"21\": \"I-OTHpart\",\n",
            "    \"22\": \"I-PER\",\n",
            "    \"23\": \"I-PERderiv\",\n",
            "    \"24\": \"I-PERpart\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 1,\n",
            "    \"B-LOCderiv\": 2,\n",
            "    \"B-LOCpart\": 3,\n",
            "    \"B-ORG\": 4,\n",
            "    \"B-ORGderiv\": 5,\n",
            "    \"B-ORGpart\": 6,\n",
            "    \"B-OTH\": 7,\n",
            "    \"B-OTHderiv\": 8,\n",
            "    \"B-OTHpart\": 9,\n",
            "    \"B-PER\": 10,\n",
            "    \"B-PERderiv\": 11,\n",
            "    \"B-PERpart\": 12,\n",
            "    \"I-LOC\": 13,\n",
            "    \"I-LOCderiv\": 14,\n",
            "    \"I-LOCpart\": 15,\n",
            "    \"I-ORG\": 16,\n",
            "    \"I-ORGderiv\": 17,\n",
            "    \"I-ORGpart\": 18,\n",
            "    \"I-OTH\": 19,\n",
            "    \"I-OTHderiv\": 20,\n",
            "    \"I-OTHpart\": 21,\n",
            "    \"I-PER\": 22,\n",
            "    \"I-PERderiv\": 23,\n",
            "    \"I-PERpart\": 24,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fhswf/bert_de_ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-LOC\",\n",
            "    \"2\": \"B-LOCderiv\",\n",
            "    \"3\": \"B-LOCpart\",\n",
            "    \"4\": \"B-ORG\",\n",
            "    \"5\": \"B-ORGderiv\",\n",
            "    \"6\": \"B-ORGpart\",\n",
            "    \"7\": \"B-OTH\",\n",
            "    \"8\": \"B-OTHderiv\",\n",
            "    \"9\": \"B-OTHpart\",\n",
            "    \"10\": \"B-PER\",\n",
            "    \"11\": \"B-PERderiv\",\n",
            "    \"12\": \"B-PERpart\",\n",
            "    \"13\": \"I-LOC\",\n",
            "    \"14\": \"I-LOCderiv\",\n",
            "    \"15\": \"I-LOCpart\",\n",
            "    \"16\": \"I-ORG\",\n",
            "    \"17\": \"I-ORGderiv\",\n",
            "    \"18\": \"I-ORGpart\",\n",
            "    \"19\": \"I-OTH\",\n",
            "    \"20\": \"I-OTHderiv\",\n",
            "    \"21\": \"I-OTHpart\",\n",
            "    \"22\": \"I-PER\",\n",
            "    \"23\": \"I-PERderiv\",\n",
            "    \"24\": \"I-PERpart\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 1,\n",
            "    \"B-LOCderiv\": 2,\n",
            "    \"B-LOCpart\": 3,\n",
            "    \"B-ORG\": 4,\n",
            "    \"B-ORGderiv\": 5,\n",
            "    \"B-ORGpart\": 6,\n",
            "    \"B-OTH\": 7,\n",
            "    \"B-OTHderiv\": 8,\n",
            "    \"B-OTHpart\": 9,\n",
            "    \"B-PER\": 10,\n",
            "    \"B-PERderiv\": 11,\n",
            "    \"B-PERpart\": 12,\n",
            "    \"I-LOC\": 13,\n",
            "    \"I-LOCderiv\": 14,\n",
            "    \"I-LOCpart\": 15,\n",
            "    \"I-ORG\": 16,\n",
            "    \"I-ORGderiv\": 17,\n",
            "    \"I-ORGpart\": 18,\n",
            "    \"I-OTH\": 19,\n",
            "    \"I-OTHderiv\": 20,\n",
            "    \"I-OTHpart\": 21,\n",
            "    \"I-PER\": 22,\n",
            "    \"I-PERderiv\": 23,\n",
            "    \"I-PERpart\": 24,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 440M/440M [00:31<00:00, 14.0MB/s]\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/model.safetensors\n",
            "Some weights of the model checkpoint at fhswf/bert_de_ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at fhswf/bert_de_ner and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([25]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([25, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 1658/1658 [00:00<00:00, 2310.70 examples/s]\n",
            "Running tokenizer on dataset: 100% 292/292 [00:00<00:00, 2508.35 examples/s]\n",
            "03/24/2024 12:59:12 - INFO - __main__ - Sample 770 of the training set: {'input_ids': [102, 5601, 4996, 30885, 125, 9710, 560, 105, 2007, 1468, 133, 221, 3625, 169, 405, 2347, 153, 12421, 148, 1768, 148, 113, 1677, 1639, 158, 128, 9162, 2036, 7665, 30886, 545, 15870, 22026, 223, 13937, 26772, 25023, 809, 919, 453, 270, 156, 223, 408, 30901, 6693, 107, 5601, 166, 8834, 162, 7104, 249, 18780, 1468, 397, 534, 30882, 9404, 30901, 25023, 809, 2415, 107, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, -100, 0, 0, -100, -100, 0, -100, 0, -100, 0, -100, -100, 0, 0, 0, 0, -100, 0, -100, 0, -100, -100, 0, 1, -100, -100, -100, 0, 0, 0, 0, 0, -100, 0, -100, 0, -100, -100, -100, 0, 0, -100, 0, -100, 0, 0, -100, -100, 0, 0, 0, -100, 0, 0, -100, 0, -100, 0, -100, 0, -100, -100]}.\n",
            "03/24/2024 12:59:12 - INFO - __main__ - Sample 1367 of the training set: {'input_ids': [102, 151, 30905, 556, 2944, 30905, 6693, 17504, 4217, 9196, 166, 8834, 408, 30901, 28324, 10075, 114, 212, 2047, 133, 221, 778, 106, 1512, 30479, 5601, 3933, 545, 7104, 2047, 305, 114, 128, 5122, 5401, 382, 22455, 2589, 30576, 4437, 25023, 809, 545, 435, 778, 30881, 9713, 30885, 128, 6090, 9787, 106, 133, 221, 125, 7732, 29695, 25023, 809, 21111, 107, 128, 670, 405, 1677, 6019, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, -100, 0, 0, -100, 0, -100, -100, -100, 0, -100, 0, -100, 0, 0, -100, 0, 0, 0, -100, 0, -100, 0, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, -100, -100, -100, -100, 0, -100, 0, -100, 0, -100, 0, -100, 0, -100, 0, 0, -100, -100, 0, -100, 0, 0, -100, 0, -100, 0, -100, 0, 0, -100, 0, -100, -100]}.\n",
            "03/24/2024 12:59:12 - INFO - __main__ - Sample 1462 of the training set: {'input_ids': [102, 151, 30905, 4729, 30882, 268, 195, 249, 15517, 1468, 4297, 5601, 125, 428, 3062, 1306, 15870, 215, 143, 22104, 23782, 9196, 25023, 809, 4268, 30882, 15870, 166, 8834, 30284, 30882, 3915, 2047, 128, 180, 30881, 15870, 215, 143, 2007, 170, 121, 988, 408, 212, 215, 30422, 107, 2460, 433, 7595, 208, 148, 27695, 155, 25023, 293, 11929, 6840, 30903, 5329, 108, 8834, 107, 5401, 30881, 6230, 4297, 494, 6165, 26772, 262, 476, 4300, 106, 6840, 5340, 121, 31056, 280, 9171, 30881, 506, 26772, 408, 30901, 18403, 30901, 108, 8834, 107, 334, 5340, 254, 31056, 123, 25023, 809, 1681, 158, 383, 631, 30284, 30882, 12440, 121, 221, 387, 2047, 506, 2432, 106, 18974, 105, 195, 249, 15517, 170, 305, 114, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, -100, 0, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, -100, -100, 0, -100, 0, -100, 0, 0, -100, 0, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, 0, -100, 0, 0, 0, 0, -100, 0, -100, -100, 0, 0, -100, -100, 0, -100, 0, 0, -100, 0, 0, -100, -100, 0, -100, 0, 0, 0, 1, -100, 0, 0, -100, -100, 0, -100, 0, -100, -100, 0, -100, 0, -100, 0, -100, 0, -100, 0, -100, -100, 0, -100, 0, -100, -100, 0, -100, 0, -100, 0, 0, 0, -100, 0, 0, -100, 0, 0, 0, -100, -100, 0, -100, 0, 0, 0, -100, 0, -100, -100]}.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:571: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
            "  warnings.warn(\n",
            "03/24/2024 12:59:14 - INFO - __main__ - ***** Running training *****\n",
            "03/24/2024 12:59:14 - INFO - __main__ -   Num examples = 1658\n",
            "03/24/2024 12:59:14 - INFO - __main__ -   Num Epochs = 3\n",
            "03/24/2024 12:59:14 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
            "03/24/2024 12:59:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "03/24/2024 12:59:14 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "03/24/2024 12:59:14 - INFO - __main__ -   Total optimization steps = 156\n",
            " 33% 52/156 [00:32<01:06,  1.56it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch 0: {'precision': 0.6615384615384615, 'recall': 0.524390243902439, 'f1': 0.5850340136054422, 'accuracy': 0.9710214154923971}\n",
            " 67% 104/156 [01:09<00:34,  1.52it/s]epoch 1: {'precision': 0.8383233532934131, 'recall': 0.8536585365853658, 'f1': 0.8459214501510574, 'accuracy': 0.9882971101026988}\n",
            "100% 156/156 [01:44<00:00,  1.56it/s]epoch 2: {'precision': 0.8574257425742574, 'recall': 0.8800813008130082, 'f1': 0.8686058174523571, 'accuracy': 0.989889340020699}\n",
            "Configuration saved in /content/our_output/config.json\n",
            "Model weights saved in /content/our_output/model.safetensors\n",
            "tokenizer config file saved in /content/our_output/tokenizer_config.json\n",
            "Special tokens file saved in /content/our_output/special_tokens_map.json\n",
            "100% 156/156 [01:49<00:00,  1.43it/s]\n"
          ]
        }
      ],
      "source": [
        "!python ner.py \\\n",
        "  --model_name_or_path fhswf/bert_de_ner \\\n",
        "  --train_file /content/ner_dataset_train.pkl \\\n",
        "  --validation_file /content/ner_dataset_eval.pkl \\\n",
        "  --text_column_name tokens \\\n",
        "  --label_column_name ner_tags \\\n",
        "  --task_name ner \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /content/our_output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OmBHIcDvNOg",
        "outputId": "c91d2eb9-e68a-4481-c7bd-23779f0bee40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 13:54:11.523434: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 13:54:11.523519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 13:54:11.524852: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 13:54:12.665138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/24/2024 13:54:16 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "Generating train split: 1658 examples [00:00, 58759.74 examples/s]\n",
            "Generating validation split: 292 examples [00:00, 50421.44 examples/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Babelscape/wikineural-multilingual-ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/tokenizer_config.json\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Babelscape--wikineural-multilingual-ner/snapshots/bed6ee7a45d2827b6c90a4fd7983f0241ae0a5c1/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Babelscape/wikineural-multilingual-ner and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 1658/1658 [00:00<00:00, 2770.51 examples/s]\n",
            "Running tokenizer on dataset: 100% 292/292 [00:00<00:00, 2559.24 examples/s]\n",
            "03/24/2024 13:54:20 - INFO - __main__ - Sample 1087 of the training set: {'input_ids': [101, 10478, 18894, 11533, 40247, 10269, 13289, 12127, 14260, 20048, 10140, 46503, 57710, 10140, 177, 10477, 173, 10361, 10123, 32118, 10118, 109628, 12926, 10104, 19962, 20873, 10123, 10143, 10305, 12228, 10123, 190, 10115, 100, 40247, 10269, 10128, 18643, 34962, 10143, 10305, 10294, 177, 10477, 190, 16497, 12059, 177, 10477, 18894, 23377, 99256, 42001, 10968, 10294, 74052, 17092, 10817, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, -100, 0, -100, 0, 0, -100, 0, 0, 0, -100, 0, 0, -100, 0, 0, -100, 0, 0, 0, -100, 0, -100, 0, -100, 0, -100, 0, -100, 0, -100, -100, 0, -100, 0, 0, -100, 0, -100, 0, 0, -100, 0, -100, -100, 0, -100, 0, -100, 0, 0, 0, 1, -100, -100, 0, -100]}.\n",
            "03/24/2024 13:54:20 - INFO - __main__ - Sample 465 of the training set: {'input_ids': [101, 10268, 12556, 14908, 10112, 10166, 10140, 36612, 10917, 10134, 10143, 10305, 45825, 46503, 41565, 10123, 10795, 33989, 84009, 10134, 46503, 67581, 10143, 10305, 10134, 10211, 81969, 10112, 59982, 45825, 11951, 33519, 10143, 10305, 10793, 10361, 10216, 10143, 10305, 11546, 67817, 10253, 37403, 10123, 10817, 10941, 10237, 10124, 15554, 66151, 13319, 45825, 36882, 10123, 10163, 10793, 10795, 10123, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, -100, -100, 0, 0, 3, -100, 0, 0, -100, 0, 0, -100, -100, 0, 0, -100, 0, 0, -100, 0, -100, 0, 0, 0, -100, 0, 0, 0, -100, 0, -100, 0, 0, -100, 0, -100, 1, -100, -100, 0, -100, 0, 0, -100, 0, 1, -100, 0, 0, 0, -100, 0, 0, 0, -100, -100]}.\n",
            "03/24/2024 13:54:20 - INFO - __main__ - Sample 165 of the training set: {'input_ids': [101, 45825, 10496, 177, 17048, 10115, 13838, 10221, 190, 95340, 10112, 190, 72975, 12059, 12729, 15329, 43125, 12979, 10140, 10104, 85980, 10223, 12979, 10279, 264, 15329, 10206, 12979, 74458, 46471, 10106, 191, 11157, 10478, 11499, 28163, 190, 10115, 100, 40247, 10269, 10478, 12228, 10123, 264, 10143, 10305, 12979, 10163, 47673, 10112, 10139, 10294, 13484, 176, 11244, 20125, 177, 17048, 184, 98240, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, -100, -100, 0, 0, 0, -100, -100, 0, -100, -100, 0, 0, -100, 0, 0, 0, -100, 0, 0, 0, 0, 0, -100, 0, 0, -100, 0, 0, -100, 0, 0, -100, 0, -100, -100, 0, -100, 0, 0, -100, 0, 0, -100, 0, 0, -100, -100, 0, 0, 0, 0, -100, -100, 0, -100, 0, -100, -100]}.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:571: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
            "  warnings.warn(\n",
            "03/24/2024 13:54:22 - INFO - __main__ - ***** Running training *****\n",
            "03/24/2024 13:54:22 - INFO - __main__ -   Num examples = 1658\n",
            "03/24/2024 13:54:22 - INFO - __main__ -   Num Epochs = 3\n",
            "03/24/2024 13:54:22 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
            "03/24/2024 13:54:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "03/24/2024 13:54:22 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "03/24/2024 13:54:22 - INFO - __main__ -   Total optimization steps = 156\n",
            " 33% 52/156 [00:32<01:05,  1.59it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch 0: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.9607159828924442}\n",
            " 67% 104/156 [01:07<00:34,  1.51it/s]epoch 1: {'precision': 0.7199170124481328, 'recall': 0.6995967741935484, 'f1': 0.7096114519427403, 'accuracy': 0.978853160145731}\n",
            "100% 156/156 [01:45<00:00,  1.45it/s]epoch 2: {'precision': 0.7777777777777778, 'recall': 0.7338709677419355, 'f1': 0.7551867219917012, 'accuracy': 0.9834468556945984}\n",
            "Configuration saved in /content/our_output/config.json\n",
            "Model weights saved in /content/our_output/model.safetensors\n",
            "tokenizer config file saved in /content/our_output/tokenizer_config.json\n",
            "Special tokens file saved in /content/our_output/special_tokens_map.json\n",
            "100% 156/156 [02:01<00:00,  1.28it/s]\n"
          ]
        }
      ],
      "source": [
        "!python ner.py \\\n",
        "  --model_name_or_path Babelscape/wikineural-multilingual-ner \\\n",
        "  --train_file /content/ner_dataset_train_lower.pkl \\\n",
        "  --validation_file /content/ner_dataset_eval_lower.pkl \\\n",
        "  --text_column_name tokens \\\n",
        "  --label_column_name ner_tags \\\n",
        "  --task_name ner \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /content/our_output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8HaMV2VvbSX",
        "outputId": "6b72b794-a98b-425a-9177-4f18a58e5002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 13:57:13.356194: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 13:57:13.356269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 13:57:13.357580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 13:57:14.479126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/24/2024 13:57:17 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fhswf/bert_de_ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fhswf/bert_de_ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-LOC\",\n",
            "    \"2\": \"B-LOCderiv\",\n",
            "    \"3\": \"B-LOCpart\",\n",
            "    \"4\": \"B-ORG\",\n",
            "    \"5\": \"B-ORGderiv\",\n",
            "    \"6\": \"B-ORGpart\",\n",
            "    \"7\": \"B-OTH\",\n",
            "    \"8\": \"B-OTHderiv\",\n",
            "    \"9\": \"B-OTHpart\",\n",
            "    \"10\": \"B-PER\",\n",
            "    \"11\": \"B-PERderiv\",\n",
            "    \"12\": \"B-PERpart\",\n",
            "    \"13\": \"I-LOC\",\n",
            "    \"14\": \"I-LOCderiv\",\n",
            "    \"15\": \"I-LOCpart\",\n",
            "    \"16\": \"I-ORG\",\n",
            "    \"17\": \"I-ORGderiv\",\n",
            "    \"18\": \"I-ORGpart\",\n",
            "    \"19\": \"I-OTH\",\n",
            "    \"20\": \"I-OTHderiv\",\n",
            "    \"21\": \"I-OTHpart\",\n",
            "    \"22\": \"I-PER\",\n",
            "    \"23\": \"I-PERderiv\",\n",
            "    \"24\": \"I-PERpart\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 1,\n",
            "    \"B-LOCderiv\": 2,\n",
            "    \"B-LOCpart\": 3,\n",
            "    \"B-ORG\": 4,\n",
            "    \"B-ORGderiv\": 5,\n",
            "    \"B-ORGpart\": 6,\n",
            "    \"B-OTH\": 7,\n",
            "    \"B-OTHderiv\": 8,\n",
            "    \"B-OTHpart\": 9,\n",
            "    \"B-PER\": 10,\n",
            "    \"B-PERderiv\": 11,\n",
            "    \"B-PERpart\": 12,\n",
            "    \"I-LOC\": 13,\n",
            "    \"I-LOCderiv\": 14,\n",
            "    \"I-LOCpart\": 15,\n",
            "    \"I-ORG\": 16,\n",
            "    \"I-ORGderiv\": 17,\n",
            "    \"I-ORGpart\": 18,\n",
            "    \"I-OTH\": 19,\n",
            "    \"I-OTHderiv\": 20,\n",
            "    \"I-OTHpart\": 21,\n",
            "    \"I-PER\": 22,\n",
            "    \"I-PERderiv\": 23,\n",
            "    \"I-PERpart\": 24,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fhswf/bert_de_ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-LOC\",\n",
            "    \"2\": \"B-LOCderiv\",\n",
            "    \"3\": \"B-LOCpart\",\n",
            "    \"4\": \"B-ORG\",\n",
            "    \"5\": \"B-ORGderiv\",\n",
            "    \"6\": \"B-ORGpart\",\n",
            "    \"7\": \"B-OTH\",\n",
            "    \"8\": \"B-OTHderiv\",\n",
            "    \"9\": \"B-OTHpart\",\n",
            "    \"10\": \"B-PER\",\n",
            "    \"11\": \"B-PERderiv\",\n",
            "    \"12\": \"B-PERpart\",\n",
            "    \"13\": \"I-LOC\",\n",
            "    \"14\": \"I-LOCderiv\",\n",
            "    \"15\": \"I-LOCpart\",\n",
            "    \"16\": \"I-ORG\",\n",
            "    \"17\": \"I-ORGderiv\",\n",
            "    \"18\": \"I-ORGpart\",\n",
            "    \"19\": \"I-OTH\",\n",
            "    \"20\": \"I-OTHderiv\",\n",
            "    \"21\": \"I-OTHpart\",\n",
            "    \"22\": \"I-PER\",\n",
            "    \"23\": \"I-PERderiv\",\n",
            "    \"24\": \"I-PERpart\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 1,\n",
            "    \"B-LOCderiv\": 2,\n",
            "    \"B-LOCpart\": 3,\n",
            "    \"B-ORG\": 4,\n",
            "    \"B-ORGderiv\": 5,\n",
            "    \"B-ORGpart\": 6,\n",
            "    \"B-OTH\": 7,\n",
            "    \"B-OTHderiv\": 8,\n",
            "    \"B-OTHpart\": 9,\n",
            "    \"B-PER\": 10,\n",
            "    \"B-PERderiv\": 11,\n",
            "    \"B-PERpart\": 12,\n",
            "    \"I-LOC\": 13,\n",
            "    \"I-LOCderiv\": 14,\n",
            "    \"I-LOCpart\": 15,\n",
            "    \"I-ORG\": 16,\n",
            "    \"I-ORGderiv\": 17,\n",
            "    \"I-ORGpart\": 18,\n",
            "    \"I-OTH\": 19,\n",
            "    \"I-OTHderiv\": 20,\n",
            "    \"I-OTHpart\": 21,\n",
            "    \"I-PER\": 22,\n",
            "    \"I-PERderiv\": 23,\n",
            "    \"I-PERpart\": 24,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"fhswf/bert_de_ner\",\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-LOC\",\n",
            "    \"2\": \"B-LOCderiv\",\n",
            "    \"3\": \"B-LOCpart\",\n",
            "    \"4\": \"B-ORG\",\n",
            "    \"5\": \"B-ORGderiv\",\n",
            "    \"6\": \"B-ORGpart\",\n",
            "    \"7\": \"B-OTH\",\n",
            "    \"8\": \"B-OTHderiv\",\n",
            "    \"9\": \"B-OTHpart\",\n",
            "    \"10\": \"B-PER\",\n",
            "    \"11\": \"B-PERderiv\",\n",
            "    \"12\": \"B-PERpart\",\n",
            "    \"13\": \"I-LOC\",\n",
            "    \"14\": \"I-LOCderiv\",\n",
            "    \"15\": \"I-LOCpart\",\n",
            "    \"16\": \"I-ORG\",\n",
            "    \"17\": \"I-ORGderiv\",\n",
            "    \"18\": \"I-ORGpart\",\n",
            "    \"19\": \"I-OTH\",\n",
            "    \"20\": \"I-OTHderiv\",\n",
            "    \"21\": \"I-OTHpart\",\n",
            "    \"22\": \"I-PER\",\n",
            "    \"23\": \"I-PERderiv\",\n",
            "    \"24\": \"I-PERpart\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 1,\n",
            "    \"B-LOCderiv\": 2,\n",
            "    \"B-LOCpart\": 3,\n",
            "    \"B-ORG\": 4,\n",
            "    \"B-ORGderiv\": 5,\n",
            "    \"B-ORGpart\": 6,\n",
            "    \"B-OTH\": 7,\n",
            "    \"B-OTHderiv\": 8,\n",
            "    \"B-OTHpart\": 9,\n",
            "    \"B-PER\": 10,\n",
            "    \"B-PERderiv\": 11,\n",
            "    \"B-PERpart\": 12,\n",
            "    \"I-LOC\": 13,\n",
            "    \"I-LOCderiv\": 14,\n",
            "    \"I-LOCpart\": 15,\n",
            "    \"I-ORG\": 16,\n",
            "    \"I-ORGderiv\": 17,\n",
            "    \"I-ORGpart\": 18,\n",
            "    \"I-OTH\": 19,\n",
            "    \"I-OTHderiv\": 20,\n",
            "    \"I-OTHpart\": 21,\n",
            "    \"I-PER\": 22,\n",
            "    \"I-PERderiv\": 23,\n",
            "    \"I-PERpart\": 24,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31102\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--fhswf--bert_de_ner/snapshots/1b14a18f1cfdafb4460fe945d27d073e8aa7ab6a/model.safetensors\n",
            "Some weights of the model checkpoint at fhswf/bert_de_ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at fhswf/bert_de_ner and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([25]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([25, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 1658/1658 [00:00<00:00, 2654.95 examples/s]\n",
            "Running tokenizer on dataset: 100% 292/292 [00:00<00:00, 2634.11 examples/s]\n",
            "03/24/2024 13:57:21 - INFO - __main__ - Sample 1402 of the training set: {'input_ids': [102, 222, 2007, 1389, 268, 5975, 4382, 30881, 276, 29019, 125, 22104, 4466, 30893, 968, 329, 15870, 10084, 4708, 433, 1677, 25780, 105, 555, 148, 1768, 408, 30901, 7920, 547, 133, 221, 9261, 2519, 573, 25023, 809, 555, 148, 30905, 16992, 133, 221, 545, 9849, 30795, 21081, 30882, 545, 128, 1458, 30881, 1631, 243, 476, 4300, 106, 128, 12889, 106, 133, 19549, 13439, 2347, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, -100, 0, -100, 0, -100, 0, 0, 0, 0, -100, -100, 0, -100, 0, 0, 0, -100, 0, -100, -100, 0, 0, -100, 0, -100, 0, -100, 0, -100, 0, 0, -100, 0, -100, 0, 0, -100, 0, 0, -100, 0, -100, 0, -100, -100, 0, 0, 0, -100, 0, -100, 0, -100, -100, 0, 0, -100, 0, -100, -100, 0, -100]}.\n",
            "03/24/2024 13:57:21 - INFO - __main__ - Sample 22 of the training set: {'input_ids': [102, 133, 221, 1677, 18971, 30881, 5527, 193, 15870, 2259, 135, 408, 30901, 545, 408, 605, 6220, 180, 2315, 5340, 8354, 106, 117, 31056, 4300, 106, 847, 1986, 2347, 30893, 408, 30901, 223, 2944, 30905, 556, 262, 121, 31056, 124, 128, 180, 153, 670, 2842, 6090, 1768, 107, 30422, 107, 556, 180, 125, 778, 106, 17514, 195, 12101, 106, 623, 1768, 107, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, -100, 0, -100, -100, 0, -100, 1, -100, -100, 0, -100, 0, 0, 0, -100, 0, 0, -100, 0, -100, 0, -100, -100, -100, 0, 0, 0, -100, 0, -100, 0, 0, -100, 0, 0, 0, -100, -100, 0, 0, 0, 0, -100, 0, -100, -100, 0, -100, 0, 0, 0, 0, -100, 0, 0, 0, -100, 0, -100, -100, -100]}.\n",
            "03/24/2024 13:57:21 - INFO - __main__ - Sample 163 of the training set: {'input_ids': [102, 133, 221, 5401, 382, 22455, 25591, 331, 125, 6693, 17504, 173, 1574, 153, 9814, 5601, 1458, 30885, 2944, 30905, 22104, 23782, 9196, 544, 15756, 9993, 9787, 30881, 9261, 125, 7920, 12725, 1729, 1512, 30479, 25023, 809, 1710, 200, 1768, 30885, 558, 548, 408, 30901, 15870, 3486, 439, 14661, 1385, 30886, 1489, 5340, 153, 408, 30901, 581, 1114, 106, 4268, 2036, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, -100, 0, -100, -100, 0, -100, 0, 0, -100, -100, -100, 0, 0, 0, 0, -100, 0, -100, 0, -100, -100, 0, -100, 0, -100, -100, 0, 0, 0, -100, -100, 0, -100, 0, -100, 0, 0, -100, -100, 0, -100, 0, -100, 0, 0, -100, 0, -100, -100, 0, -100, 0, 0, -100, 1, -100, -100, 0, -100, -100]}.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:571: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
            "  warnings.warn(\n",
            "03/24/2024 13:57:22 - INFO - __main__ - ***** Running training *****\n",
            "03/24/2024 13:57:22 - INFO - __main__ -   Num examples = 1658\n",
            "03/24/2024 13:57:22 - INFO - __main__ -   Num Epochs = 3\n",
            "03/24/2024 13:57:22 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
            "03/24/2024 13:57:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "03/24/2024 13:57:22 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "03/24/2024 13:57:22 - INFO - __main__ -   Total optimization steps = 156\n",
            " 33% 52/156 [00:33<01:07,  1.53it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch 0: {'precision': 0.9166666666666666, 'recall': 0.08943089430894309, 'f1': 0.16296296296296298, 'accuracy': 0.9640952153490964}\n",
            " 67% 104/156 [01:10<00:33,  1.54it/s]epoch 1: {'precision': 0.8146551724137931, 'recall': 0.7682926829268293, 'f1': 0.7907949790794979, 'accuracy': 0.9853514847543985}\n",
            "100% 156/156 [01:45<00:00,  1.54it/s]epoch 2: {'precision': 0.83640081799591, 'recall': 0.8313008130081301, 'f1': 0.8338430173292558, 'accuracy': 0.9876602181354988}\n",
            "Configuration saved in /content/our_output/config.json\n",
            "Model weights saved in /content/our_output/model.safetensors\n",
            "tokenizer config file saved in /content/our_output/tokenizer_config.json\n",
            "Special tokens file saved in /content/our_output/special_tokens_map.json\n",
            "100% 156/156 [01:50<00:00,  1.41it/s]\n"
          ]
        }
      ],
      "source": [
        "!python ner.py \\\n",
        "  --model_name_or_path fhswf/bert_de_ner \\\n",
        "  --train_file /content/ner_dataset_train_lower.pkl \\\n",
        "  --validation_file /content/ner_dataset_eval_lower.pkl \\\n",
        "  --text_column_name tokens \\\n",
        "  --label_column_name ner_tags \\\n",
        "  --task_name ner \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /content/our_output/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
